{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abce442",
   "metadata": {},
   "source": [
    "# Cell 1: Markdown\n",
    "\"\"\"\n",
    "# mRNA Classification Project\n",
    "**Course:** MSI5001 - Introduction to AI  \n",
    "**Team Members:** Lisa Mithani, Shawn Lee, Aishwarya Nair, and Kalyani Vijay\n",
    "**Dataset:** mRNA Classification (Medium difficulty)  \n",
    "**Objective:** Classify RNA sequences as mRNA vs. other RNA types using machine learning models\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Data Loading & Merging\n",
    "2. Exploratory Data Analysis\n",
    "3. Feature Extraction\n",
    "4. Model Training\n",
    "5. Evaluation & Insights\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb909769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /opt/homebrew/anaconda3/lib/python3.13/site-packages (1.86)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from biopython) (2.2.5)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/homebrew/anaconda3/lib/python3.13/site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from imbalanced-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from imbalanced-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from imbalanced-learn) (1.7.1)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/anaconda3/lib/python3.13/site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio in /opt/homebrew/anaconda3/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/anaconda3/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/anaconda3/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython\n",
    "!pip install imbalanced-learn\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd85584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859d632",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Data Loading and Preparation\n",
    "**Responsible:** Lisa Mithani\n",
    "\n",
    "## Overview\n",
    "This section loads the training and testing datasets, merges them, and prepares features and labels for modeling.\n",
    "\n",
    "## Tasks:\n",
    "1. Load training.fa (FASTA) and training.csv → merge using inner join\n",
    "2. Load testing.csv\n",
    "3. Separate features (X) and labels (y) for train/test sets\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "215f0bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training.fa...\n",
      "✓ Loaded 22867 sequences from training.fa\n",
      "\n",
      "Loading training.csv...\n",
      "✓ Loaded training.csv with shape (22867, 2)\n",
      "\n",
      "Merging datasets using inner join...\n",
      "✓ Merged training data: (14286, 4)\n",
      "  Columns: ['sequence_id', 'sequence', 'name', 'class']\n",
      "\n",
      "✓ STEP 1 COMPLETE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSDART00000138379</td>\n",
       "      <td>TCAAANGGAAAATAATATGTCAGYTGTGATTTTTACTCGANTTAAT...</td>\n",
       "      <td>ENSDART00000138379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSDART00000075994</td>\n",
       "      <td>ATGTCTCTTTTTGAAATAAAAGACCTGNTTNGAGAAGGAAGCTATG...</td>\n",
       "      <td>ENSDART00000075994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sequence_id                                           sequence  \\\n",
       "0  ENSDART00000138379  TCAAANGGAAAATAATATGTCAGYTGTGATTTTTACTCGANTTAAT...   \n",
       "1  ENSDART00000075994  ATGTCTCTTTTTGAAATAAAAGACCTGNTTNGAGAAGGAAGCTATG...   \n",
       "\n",
       "                 name  class  \n",
       "0  ENSDART00000138379      1  \n",
       "1  ENSDART00000075994      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Load training.fa and training.csv, then merge using inner join\n",
    "# ============================================================================\n",
    "\n",
    "# Function to load FASTA file\n",
    "def load_fasta_to_dataframe(fasta_file):\n",
    "    \"\"\"\n",
    "    Reads a FASTA file and converts it to a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - fasta_file: path to .fa file\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns: ['sequence_id', 'sequence']\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append({\n",
    "            'sequence_id': record.id,\n",
    "            'sequence': str(record.seq)\n",
    "        })\n",
    "    return pd.DataFrame(sequences)\n",
    "\n",
    "# Load training.fa (FASTA file)\n",
    "print(\"Loading training.fa...\")\n",
    "fasta_df = load_fasta_to_dataframe('dataset/training.fa')\n",
    "print(f\"✓ Loaded {fasta_df.shape[0]} sequences from training.fa\")\n",
    "\n",
    "# Load training.csv\n",
    "print(\"\\nLoading training.csv...\")\n",
    "csv_df = pd.read_csv('dataset/training_class.csv')\n",
    "print(f\"✓ Loaded training.csv with shape {csv_df.shape}\")\n",
    "\n",
    "# Merge using inner join with DIFFERENT column names\n",
    "# FASTA has 'sequence_id', CSV has 'name'\n",
    "print(\"\\nMerging datasets using inner join...\")\n",
    "training_data = pd.merge(\n",
    "    fasta_df,\n",
    "    csv_df,\n",
    "    left_on='sequence_id',   # Column in FASTA\n",
    "    right_on='name',          # Column in CSV (different name!)\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Merged training data: {training_data.shape}\")\n",
    "print(f\"  Columns: {list(training_data.columns)}\")\n",
    "print(f\"\\n✓ STEP 1 COMPLETE\")\n",
    "\n",
    "# Display first few rows\n",
    "training_data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6fbce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing.csv...\n",
      "✓ Loaded testing.csv: (4416, 3)\n",
      "  Columns: ['name', 'sequence', 'class']\n",
      "\n",
      "✓ STEP 2 COMPLETE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sequence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCONS_00059596</td>\n",
       "      <td>CUAAUCCCCCCUCCUCCCGCUCCCGCACCAAAGAGUUGCGCCGCCU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCONS_00059678</td>\n",
       "      <td>CUAUUCGGCGCAGUUGCUAUACGUACCCCAGCCUCGUACACAACGC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                           sequence  class\n",
       "0  TCONS_00059596  CUAAUCCCCCCUCCUCCCGCUCCCGCACCAAAGAGUUGCGCCGCCU...      1\n",
       "1  TCONS_00059678  CUAUUCGGCGCAGUUGCUAUACGUACCCCAGCCUCGUACACAACGC...      1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load testing.csv\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading testing.csv...\")\n",
    "testing_data = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "print(f\"✓ Loaded testing.csv: {testing_data.shape}\")\n",
    "print(f\"  Columns: {list(testing_data.columns)}\")\n",
    "print(f\"\\n✓ STEP 2 COMPLETE\")\n",
    "\n",
    "testing_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e331e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating training data...\n",
      "✓ X_train shape: (14286, 1)\n",
      "  Columns: ['sequence']\n",
      "✓ y_train shape: (14286,)\n",
      "  Class distribution:\n",
      "class\n",
      "0    9224\n",
      "1    5062\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Separating testing data...\n",
      "✓ X_test shape: (4416, 1)\n",
      "  Columns: ['sequence']\n",
      "✓ y_test shape: (4416,)\n",
      "  Class distribution:\n",
      "class\n",
      "1    2208\n",
      "0    2208\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ STEP 3 COMPLETE\n",
      "\n",
      "⚠️ Note: X_train and X_test contain raw sequences.\n",
      "   Next step: Feature extraction (k-mer generation)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Drop class labels and store them in y_train and y_test\n",
    "# ============================================================================\n",
    "\n",
    "label_column = 'class'\n",
    "\n",
    "# --- Training Set ---\n",
    "print(\"Separating training data...\")\n",
    "y_train = training_data[label_column].copy()\n",
    "# Keep 'sequence' column for next teammate to extract features\n",
    "X_train = training_data.drop(columns=[label_column, 'sequence_id', 'name']).copy()\n",
    "\n",
    "print(f\"✓ X_train shape: {X_train.shape}\")\n",
    "print(f\"  Columns: {list(X_train.columns)}\")  # Should show ['sequence']\n",
    "print(f\"✓ y_train shape: {y_train.shape}\")\n",
    "print(f\"  Class distribution:\\n{y_train.value_counts()}\")\n",
    "\n",
    "# --- Testing Set ---\n",
    "print(\"\\nSeparating testing data...\")\n",
    "y_test = testing_data[label_column].copy()\n",
    "# Keep 'sequence' column for next teammate\n",
    "X_test = testing_data.drop(columns=[label_column, 'name']).copy()\n",
    "\n",
    "print(f\"✓ X_test shape: {X_test.shape}\")\n",
    "print(f\"  Columns: {list(X_test.columns)}\")  # Should show ['sequence']\n",
    "print(f\"✓ y_test shape: {y_test.shape}\")\n",
    "print(f\"  Class distribution:\\n{y_test.value_counts()}\")\n",
    "\n",
    "print(f\"\\n✓ STEP 3 COMPLETE\")\n",
    "print(f\"\\n⚠️ Note: X_train and X_test contain raw sequences.\")\n",
    "print(f\"   Next step: Feature extraction (k-mer generation)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560c6b6",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 Complete ✓\n",
    "\n",
    "**Outputs:**\n",
    "- `X_train` (14,286 samples × 1 column): Raw RNA sequences\n",
    "- `y_train` (14,286 labels): Class labels (0 or 1)\n",
    "- `X_test` (4,416 samples × 1 column): Raw RNA sequences\n",
    "- `y_test` (4,416 labels): Class labels (0 or 1)\n",
    "\n",
    "**Note for Next Teammate:**\n",
    "The `X_train` and `X_test` DataFrames contain raw RNA sequences in the `'sequence'` column. Please convert these to numeric features (e.g., k-mer frequencies) before model training.\n",
    "\n",
    "**Next Section:** Feature Extraction / Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69271ee0",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Feature Extraction\n",
    "**Responsible:** [Teammate Names TBD]\n",
    "\n",
    "## Overview\n",
    "This section converts raw RNA sequences into three types of numeric features for modeling.\n",
    "\n",
    "We will extract:\n",
    "1. Character Positional Embeddings\n",
    "2. Character Tokenization\n",
    "3. K-mer Frequency Features\n",
    "\n",
    "Each feature type will be used to train separate models, then compared.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e19286",
   "metadata": {},
   "source": [
    "### Step 2.1: Character Positional Embedding\n",
    "Convert RNA sequences to positional embeddings using learned vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2.1: Character Positional Embedding - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Convert X_train['sequence'] and X_test['sequence'] to positional embeddings\n",
    "# Output: X_train_embedding, X_test_embedding (numeric DataFrames)\n",
    "\n",
    "# Example approach:\n",
    "# - Map each nucleotide (A, U, G, C) to a position index\n",
    "# - Create embedding vectors for each position\n",
    "# - Concatenate or average embeddings across sequence length\n",
    "\n",
    "print(\"⚠️ Step 2.1 incomplete - waiting for implementation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8344d21",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2.2: Character Tokenization\n",
    "Tokenize sequences into character-level representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2.2: Character Tokenization - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Tokenize X_train['sequence'] and X_test['sequence']\n",
    "# Output: X_train_tokens, X_test_tokens (numeric DataFrames)\n",
    "\n",
    "# Example approach:\n",
    "# - Create vocabulary: {'A': 0, 'U': 1, 'G': 2, 'C': 3}\n",
    "# - Convert each sequence to list of token IDs\n",
    "# - Pad sequences to same length\n",
    "# - Convert to numeric array\n",
    "\n",
    "print(\"⚠️ Step 2.2 incomplete - waiting for implementation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc845ae",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2.3: K-mer Feature Extraction\n",
    "Extract k-mer frequency features (k=3, 4, or 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee98fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2.3: K-mer Feature Extraction - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Extract k-mer features from X_train['sequence'] and X_test['sequence']\n",
    "# Output: X_train_kmers, X_test_kmers (numeric DataFrames)\n",
    "\n",
    "# OPTION A: Extract k-mers manually\n",
    "# - Loop through sequences and extract overlapping k-mers\n",
    "# - Count k-mer frequencies\n",
    "# - Normalize to create feature vectors\n",
    "\n",
    "# OPTION B: Load pre-computed features (FASTER)\n",
    "# X_train_kmers = pd.read_csv('dataset/train_kmer_features_k3.csv')\n",
    "\n",
    "print(\"⚠️ Step 2.3 incomplete - waiting for implementation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37012dfb",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 Summary\n",
    "\n",
    "**Expected Outputs:**\n",
    "- `X_train_embedding` (14,286 samples × embedding_dim features)\n",
    "- `X_test_embedding` (4,416 samples × embedding_dim features)\n",
    "- `X_train_tokens` (14,286 samples × sequence_length features)\n",
    "- `X_test_tokens` (4,416 samples × sequence_length features)\n",
    "- `X_train_kmers` (14,286 samples × n_kmers features)\n",
    "- `X_test_kmers` (4,416 samples × n_kmers features)\n",
    "\n",
    "All features are now numeric and ready for class balancing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3a27b",
   "metadata": {},
   "source": [
    "# 3. Class Balancing\n",
    "**Responsible:** Shawn Lee\n",
    "\n",
    "## Overview\n",
    "Balance class distribution for each feature type separately.\n",
    "\n",
    "Current training class distribution:\n",
    "- Class 0: 9,224 samples (64.5%)\n",
    "- Class 1: 5,062 samples (35.5%)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a function that will balance the databset passed in\n",
    "# Input: X_train, y_train\n",
    "# Output: X_train_balanced, y_train_balanced\n",
    "\n",
    "# Using SMOTE:\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# Usage: X_train_embedding_balanced, y_train_balanced = smote.fit_resample(X_train_embedding, y_train)\n",
    "\n",
    "def balance_features(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    print(f\"✓ Balanced features: {X_balanced.shape}, {y_balanced.shape}\")\n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cad48c",
   "metadata": {},
   "source": [
    "### Step 3.1: Balance Embedding Features COMPLETE ✓\n",
    "Apply class balancing to positional embedding features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3.1: Balance Embedding Features - COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Balance X_train_embedding with y_train\n",
    "# New datasets: X_train_embedding_balanced, y_train_balanced\n",
    "\n",
    "#X_train_embedding_balanced, y_train_balanced = balance_features(X_train_embedding, y_train)\n",
    "#print(\"✓ Step 3.1 complete - Embedding features balanced.\")\n",
    "\n",
    "print(\"⚠️ Step 3.2 incomplete - waiting for dataset to be available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302ae2c",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3.2: Balance Token Features\n",
    "Apply class balancing to tokenization features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3.2: Balance Token Features - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Balance X_train_tokens with y_train\n",
    "# New datasets: X_train_tokens_balanced\n",
    "\n",
    "#X_train_tokens_balanced, y_train_balanced = balance_features(X_train_tokens, y_train)\n",
    "#print(\"✓ Step 3.2 complete - Token features balanced.\")\n",
    "\n",
    "print(\"⚠️ Step 3.2 incomplete - waiting for dataset to be available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c0049",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3.3: Balance K-mer Features\n",
    "Apply class balancing to k-mer frequency features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3.3: Balance K-mer Features - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Balance X_train_kmers with y_train\n",
    "# New datasets: X_train_kmers_balanced\n",
    "\n",
    "#X_train_kmers_balanced, y_train_balanced = balance_features(X_train_kmers, y_train)\n",
    "#print(\"✓ Step 3.3 complete - K-mer features balanced.\")\n",
    "\n",
    "print(\"⚠️ Step 3.3 incomplete - waiting for dataset to be available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a1cf7",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 Summary\n",
    "\n",
    "All three feature types are now balanced and ready for modeling.\n",
    "\n",
    "**Balanced Datasets:**\n",
    "- Embedding features: X_train_embedding_balanced, y_train_balanced\n",
    "- Token features: X_train_tokens_balanced, y_train_balanced\n",
    "- K-mer features: X_train_kmers_balanced, y_train_balanced\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ac189",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Model Training\n",
    "**Responsible:** [Teammate Names TBD]\n",
    "\n",
    "## Overview\n",
    "Train three separate logistic regression models using each feature type:\n",
    "1. Model A: Using Positional Embedding features\n",
    "2. Model B: Using Tokenization features\n",
    "3. Model C: Using K-mer features\n",
    "\n",
    "Each model will be evaluated independently to compare feature effectiveness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e66fd",
   "metadata": {},
   "source": [
    "### Step 4.1: Train Logistic Regression Model A (Embedding Features)\n",
    "Train logistic regression on positional embedding features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4.1: Train Model A (Embedding Features) - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Train logistic regression on X_train_embedding_balanced\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# \n",
    "# model_A = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# model_A.fit(X_train_embedding_balanced, y_train_balanced)\n",
    "# \n",
    "# # Predict on test set\n",
    "# y_pred_A = model_A.predict(X_test_embedding)\n",
    "#\n",
    "# Output: model_A, y_pred_A\n",
    "\n",
    "print(\"⚠️ Step 4.1 incomplete - Model A training pending\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f3a33",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4.2: Train Logistic Regression Model B (Token Features)\n",
    "Train logistic regression on character tokenization features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc942aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4.2: Train Model B (Token Features) - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Train logistic regression on X_train_tokens_balanced\n",
    "# model_B = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# model_B.fit(X_train_tokens_balanced, y_train_balanced)\n",
    "#\n",
    "# y_pred_B = model_B.predict(X_test_tokens)\n",
    "#\n",
    "# Output: model_B, y_pred_B\n",
    "\n",
    "print(\"⚠️ Step 4.2 incomplete - Model B training pending\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79230cd6",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4.3: Train Logistic Regression Model C (K-mer Features)\n",
    "Train logistic regression on k-mer frequency features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e216dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4.3: Train Model C (K-mer Features) - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Train logistic regression on X_train_kmers_balanced\n",
    "# model_C = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# model_C.fit(X_train_kmers_balanced, y_train_balanced)\n",
    "#\n",
    "# y_pred_C = model_C.predict(X_test_kmers)\n",
    "#\n",
    "# Output: model_C, y_pred_C\n",
    "\n",
    "print(\"⚠️ Step 4.3 incomplete - Model C training pending\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e81ee",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 Summary\n",
    "\n",
    "**Trained Models:**\n",
    "- Model A: Logistic Regression on Embedding Features\n",
    "- Model B: Logistic Regression on Token Features\n",
    "- Model C: Logistic Regression on K-mer Features\n",
    "\n",
    "All models trained and ready for evaluation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfb3f9",
   "metadata": {},
   "source": [
    "# 5. Classification Performance Evaluation\n",
    "**Responsible:** Shawn Lee\n",
    "\n",
    "## Overview\n",
    "Evaluate and compare the performance of all three logistic regression models.\n",
    "\n",
    "## Metrics:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- MCC\n",
    "- Classification Report\n",
    "- Confusion Matrix\n",
    "- ROC-AUC\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8619e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Creation of evaluation functions\n",
    "# Usage: test_metrics = compute_metrics(y_true, y_pred)\n",
    "#        print_metrics(test_metrics, model_name, trained_model)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute sensitivity, specificity, and MCC for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (0 or 1)\n",
    "        y_pred: Predicted labels (0 or 1)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with sensitivity, specificity, mcc, tp, tn, fp, fn, precision, f1-score, recall\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Sensitivity (Recall, True Positive Rate)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    # Specificity (True Negative Rate)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Calculate Precision and F1-score and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0.0\n",
    "    recall = sensitivity\n",
    "\n",
    "    return {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'mcc': mcc,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix using matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (0 or 1)\n",
    "        y_pred: Predicted labels (0 or 1)\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "    # Show all ticks and label them\n",
    "    ax.set_xticks(np.arange(cm.shape[1]))\n",
    "    ax.set_yticks(np.arange(cm.shape[0]))\n",
    "    ax.set_xticklabels(['0', '1'])\n",
    "    ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            color = \"white\" if cm[i, j] > cm.max() / 2 else \"black\"\n",
    "            ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=color)\n",
    "\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.colorbar(im)\n",
    "    plt.show()\n",
    "\n",
    "def plot_ROC_Curve(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Plot an interpretable ROC Curve with AUC and the threshold that maximizes MCC.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels (0 or 1)\n",
    "        y_scores: Predicted probabilities for the positive class\n",
    "\n",
    "    Returns:\n",
    "        (fig, ax, auc, best_t, best_mcc) - useful for further programmatic use\n",
    "    \"\"\"\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "    auc_score = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    # Search thresholds (include 0/1 extremes) and pick best by MCC\n",
    "    search_thresholds = np.linspace(0.0, 1.0, 101)\n",
    "    best_mcc = -1.0\n",
    "    best_t = 0.5\n",
    "    for t in search_thresholds:\n",
    "        preds_t = (y_scores >= t).astype(int)\n",
    "        try:\n",
    "            mcc_t = matthews_corrcoef(y_true, preds_t)\n",
    "        except Exception:\n",
    "            mcc_t = -1.0\n",
    "        if mcc_t > best_mcc:\n",
    "            best_mcc = mcc_t\n",
    "            best_t = t\n",
    "\n",
    "    # Compute FPR/TPR for best threshold (from confusion matrix)\n",
    "    cm = confusion_matrix(y_true, (y_scores >= best_t).astype(int), labels=[0, 1])\n",
    "    if cm.size == 4:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        fpr_best = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        tpr_best = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    else:\n",
    "        # fallback\n",
    "        fpr_best, tpr_best = 0.0, 0.0\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    ax.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {auc_score:.4f})', color='tab:blue')\n",
    "    ax.fill_between(fpr, tpr, alpha=0.15, color='tab:blue')\n",
    "    ax.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random (AUC=0.5)')\n",
    "\n",
    "    # Mark and annotate best threshold point\n",
    "    ax.scatter([fpr_best], [tpr_best], color='red', zorder=5)\n",
    "    ax.annotate(f't={best_t:.3f}\\nMCC={best_mcc:.3f}\\nTPR={tpr_best:.3f}\\nFPR={fpr_best:.3f}',\n",
    "                xy=(fpr_best, tpr_best),\n",
    "                xytext=(fpr_best + 0.05, tpr_best - 0.12),\n",
    "                bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.9),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1),\n",
    "                fontsize=9)\n",
    "\n",
    "    # Formatting for interpretability\n",
    "    ax.set_xlim([-0.01, 1.01])\n",
    "    ax.set_ylim([-0.01, 1.01])\n",
    "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "    ax.set_ylabel('True Positive Rate (Sensitivity / Recall)')\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax.grid(True, linestyle=':', alpha=0.6)\n",
    "    ax.legend(loc='lower right', framealpha=0.9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax, auc_score, best_t, best_mcc\n",
    "\n",
    "def print_metrics(test_metrics, model_name, trained_model=None):\n",
    "    \"\"\"\n",
    "    Print the evaluation metrics in a formatted way.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary of metrics\n",
    "        model_name: Name of the model being evaluated (string)\n",
    "        trained_model: (optional) trained model to get predicted probabilities\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{model_name} MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy:                  {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Specificity (Precision/TNR):        {test_metrics['specificity']:.4f}\")\n",
    "    print(f\"Sensitivity (Recall/TPR): {test_metrics['sensitivity']:.4f}\")\n",
    "    print(f\"F1 Score:                {test_metrics['f1_score']:.4f}\")\n",
    "    print(f\"MCC:                      {test_metrics['mcc']:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_metrics['y_true'], test_metrics['y_pred']))\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  True Positives  (TP): {test_metrics['tp']:5d}\")\n",
    "    print(f\"  True Negatives  (TN): {test_metrics['tn']:5d}\")\n",
    "    print(f\"  False Positives (FP): {test_metrics['fp']:5d}\")\n",
    "    print(f\"  False Negatives (FN): {test_metrics['fn']:5d}\")\n",
    "    print(f\"  Precision:               {test_metrics['precision']:.4f}\")\n",
    "    print(f\"  F1 Score:                {test_metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Recall:                  {test_metrics['recall']:.4f}\")\n",
    "    print(f\"  Total Samples:        {test_metrics['tp'] + test_metrics['tn'] + test_metrics['fp'] + test_metrics['fn']:5d}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(test_metrics['y_true'], test_metrics['y_pred'])\n",
    "    # Plot ROC Curve\n",
    "    test_probs = trained_model.predict_proba(X_test)[:, 1] if trained_model else test_metrics['y_pred']\n",
    "    plot_ROC_Curve(test_metrics['y_true'], test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce5655",
   "metadata": {},
   "source": [
    "### Step 5.1: Evaluate All Models\n",
    "Calculate performance metrics for Models A, B, and C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d41edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.1: Evaluate All Models - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Evaluate each model's performance\n",
    "#\n",
    "# # Evaluate Model A\n",
    "# test_metrics_A = compute_metrics(y_test, y_pred_A)\n",
    "# print_metrics(test_metrics_A, \"Model A (Embedding)\", trained_model=model_A)\n",
    "#\n",
    "# # Repeat for Models B and C\n",
    "#\n",
    "# # Create comparison DataFrame\n",
    "# results = pd.DataFrame({\n",
    "#     'Model': ['Model A (Embedding)', 'Model B (Tokens)', 'Model C (K-mers)'],\n",
    "#     'Accuracy': [acc_A, acc_B, acc_C],\n",
    "#     'Precision': [precision_A, precision_B, precision_C],\n",
    "#     'Recall': [recall_A, recall_B, recall_C],\n",
    "#     'F1-Score': [f1_A, f1_B, f1_C]\n",
    "# })\n",
    "#\n",
    "# print(results)\n",
    "\n",
    "print(\"⚠️ Step 5.1 incomplete - Model evaluation pending trained models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f4b56",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5.2: Compare Model Performance\n",
    "Visualize and compare performance across all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e90705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.2: Compare Model Performance - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Create comparison visualizations\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "#\n",
    "# # Bar chart comparing metrics\n",
    "# results.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score']].plot(kind='bar', figsize=(10,6))\n",
    "# plt.title('Model Performance Comparison')\n",
    "# plt.ylabel('Score')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "print(\"⚠️ Step 5.2 incomplete - Performance comparison pending trained models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2c488",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5.3: Select Best Model\n",
    "Identify the best-performing model based on evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc914759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.3: Select Best Model - TO BE COMPLETED\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Determine best model\n",
    "# best_model_idx = results['F1-Score'].idxmax()\n",
    "# best_model_name = results.loc[best_model_idx, 'Model']\n",
    "# best_f1 = results.loc[best_model_idx, 'F1-Score']\n",
    "#\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(f\"BEST MODEL: {best_model_name}\")\n",
    "# print(f\"F1-Score: {best_f1:.4f}\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "print(\"⚠️ Step 5.3 incomplete - Best model selection pending trained models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca87d3",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 Summary\n",
    "\n",
    "**Performance Comparison Complete:**\n",
    "- All three models evaluated using standard classification metrics\n",
    "- Best performing model identified\n",
    "- Results ready for reporting\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862f06c",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Advanced Model Experiments\n",
    "**Team Contributions:** Individual advanced model implementations\n",
    "\n",
    "## Overview\n",
    "After baseline logistic regression comparison, each team member explores an advanced model with custom preprocessing to maximize performance.\n",
    "\n",
    "## Team Member Assignments:\n",
    "- **Lisa:** Random Forest Classifier\n",
    "- **Shawn:** Recurrent Neural Network (RNN)\n",
    "- **Aishwarya:** Long Short-Term Memory (LSTM)\n",
    "- **Kalyani:** Transformer Model\n",
    "\n",
    "Each subsection contains: Preprocessing → Model Training → Performance Evaluation (all in one cell).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933a00f",
   "metadata": {},
   "source": [
    "## 6.1 Random Forest Classifier\n",
    "**Responsible:** Lisa\n",
    "\n",
    "### Approach\n",
    "Use Random Forest with k-mer features and custom preprocessing.\n",
    "\n",
    "This cell contains: Additional preprocessing → Model training → Performance evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fe907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b189e66",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.2 Recurrent Neural Network (RNN)\n",
    "**Responsible:** Shawn\n",
    "\n",
    "### Approach\n",
    "Use RNN with sequential token features to capture sequence patterns.\n",
    "\n",
    "This cell contains: Additional preprocessing → Model training → Performance evaluation.\n",
    "\n",
    "1st version of codes in this cell was generated usig ChatGPT with prompt \"provide codes to train a RNN model to classify RNA dataset in python assuming that the dataset that requires tokenization and class balancing\". Subsequently, changes were done to ensure correctness and alignment to codes in other cells in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6.2: Recurrent Neural Network (RNN) MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SECTION 6.2: RNN MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n✓ Using device: {device}\")\n",
    "\n",
    "#### STARTING FROM HERE, CODES TO BE REMOVED ONCE TOKENIZATION IS DONE ABOVE. TAKE NOTE TO USE BALANCED DATASET ####\n",
    "\n",
    "print(\"\\n[Step 1/4] Tokenizing sequences...\")\n",
    "\n",
    "# Define vocabulary for RNA nucleotides\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "nucleotides = sorted(list(set('ACGTURYSWKMBDHVN')))  # IUPAC alphabet\n",
    "char2idx = {ch: i + 2 for i, ch in enumerate(nucleotides)}\n",
    "char2idx['<PAD>'] = PAD_IDX\n",
    "char2idx['<UNK>'] = UNK_IDX\n",
    "vocab_size = len(char2idx)\n",
    "\n",
    "print(f\"✓ Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def tokenize_sequence(seq, char2idx, max_len=512):\n",
    "    \"\"\"Convert sequence string to list of token indices.\"\"\"\n",
    "    tokens = [char2idx.get(char, UNK_IDX) for char in seq[:max_len]]\n",
    "    # Pad to max_len\n",
    "    if len(tokens) < max_len:\n",
    "        tokens = tokens + [PAD_IDX] * (max_len - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "# Determine max sequence length (cap at 512 for memory)\n",
    "max_seq_len = min(512, max(X_train['sequence'].str.len().max(), \n",
    "                             X_test['sequence'].str.len().max()))\n",
    "print(f\"✓ Max sequence length: {max_seq_len}\")\n",
    "\n",
    "# Tokenize all sequences (assigning to X_train_tokens & X_test_tokens to align with unfinished codes above. To update again if variable name changes.)\n",
    "X_train_tokens_balanced = X_train['sequence'].apply(\n",
    "    lambda s: tokenize_sequence(s, char2idx, max_seq_len)\n",
    ").tolist()\n",
    "X_test_tokens = X_test['sequence'].apply(\n",
    "    lambda s: tokenize_sequence(s, char2idx, max_seq_len)\n",
    ").tolist()\n",
    "\n",
    "y_train_balanced = y_train  # Placeholder until balancing is done above\n",
    "#### END OF REMOVAL ####\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_tokens_balanced, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_tokens, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train_balanced.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "print(f\"✓ Training tensor shape: {X_train_tensor.shape}\")\n",
    "print(f\"✓ Test tensor shape: {X_test_tensor.shape}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create DataLoaders\n",
    "# ----------------------------------------------------------------------------\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"✓ Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Define RNN Model\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_size=128, \n",
    "                 num_layers=2, dropout=0.3, pad_idx=PAD_IDX):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # RNN layer (GRU for better gradient flow)\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim, \n",
    "            hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 64)  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, 2)  # Binary classification\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        rnn_out, hidden = self.rnn(embedded)  # rnn_out: (batch, seq_len, hidden*2)\n",
    "        \n",
    "        # Use last hidden state (concatenate forward and backward)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # (batch, hidden*2)\n",
    "        else:\n",
    "            hidden = hidden[-1]  # (batch, hidden)\n",
    "        \n",
    "        # Classification head\n",
    "        out = self.relu(self.fc1(hidden))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Instantiate model\n",
    "model = RNNClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(f\"✓ Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training Setup\n",
    "# ----------------------------------------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training Loop\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\nTraining RNN model...\")\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "train_losses, val_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_rnn_model.pth')\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Loss: {avg_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_rnn_model.pth'))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Evaluation on Test Set\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RNN MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1\n",
    "\n",
    "y_pred_rnn = np.array(all_preds)\n",
    "y_probs_rnn = np.array(all_probs)\n",
    "\n",
    "# Compute metrics using existing function\n",
    "test_metrics_rnn = compute_metrics(y_test.values, y_pred_rnn)\n",
    "test_metrics_rnn['accuracy'] = (test_metrics_rnn['tp'] + test_metrics_rnn['tn']) / len(y_test)\n",
    "\n",
    "# Print detailed metrics\n",
    "print_metrics(test_metrics_rnn, \"RNN\", trained_model=None)\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Training Loss', color='tab:blue')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(val_accs, label='Validation Accuracy', color='tab:orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ RNN MODEL SECTION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e8dd5",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.3 Long Short-Term Memory (LSTM)\n",
    "**Responsible:** Aishwarya\n",
    "\n",
    "### Approach\n",
    "Use LSTM to capture long-range dependencies in RNA sequences.\n",
    "\n",
    "This cell contains: Additional preprocessing → Model training → Performance evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4128715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c54d29e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.4 Transformer Model\n",
    "**Responsible:** Kalyani\n",
    "\n",
    "### Approach\n",
    "Use Transformer architecture with attention mechanism for sequence classification.\n",
    "\n",
    "This cell contains: Additional preprocessing → Model training → Performance evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd63a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b530de5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.5 Comprehensive Model Comparison\n",
    "Compare all models: Baseline Logistic Regression + Advanced Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797beb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5354f29f",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 Complete ✓\n",
    "\n",
    "**Advanced Models Implemented:**\n",
    "- ✅ Random Forest (Lisa)\n",
    "- ✅ RNN (Shawn)\n",
    "- ✅ LSTM (Aishwarya)\n",
    "- ✅ Transformer (Kalyani)\n",
    "- ✅ Comprehensive comparison of all 7 models\n",
    "\n",
    "**Key Findings:** [To be filled after implementation]\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
