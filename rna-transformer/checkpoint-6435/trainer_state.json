{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 6435,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0777000777000777,
      "grad_norm": 2.0893502235412598,
      "learning_rate": 4.923076923076924e-05,
      "loss": 0.6762,
      "step": 100
    },
    {
      "epoch": 0.1554001554001554,
      "grad_norm": 1.1986085176467896,
      "learning_rate": 4.845376845376846e-05,
      "loss": 0.6599,
      "step": 200
    },
    {
      "epoch": 0.2331002331002331,
      "grad_norm": 6.366525650024414,
      "learning_rate": 4.7676767676767684e-05,
      "loss": 0.6247,
      "step": 300
    },
    {
      "epoch": 0.3108003108003108,
      "grad_norm": 15.028244972229004,
      "learning_rate": 4.68997668997669e-05,
      "loss": 0.4932,
      "step": 400
    },
    {
      "epoch": 0.3885003885003885,
      "grad_norm": 12.027316093444824,
      "learning_rate": 4.612276612276613e-05,
      "loss": 0.4617,
      "step": 500
    },
    {
      "epoch": 0.4662004662004662,
      "grad_norm": 4.73768949508667,
      "learning_rate": 4.534576534576535e-05,
      "loss": 0.4292,
      "step": 600
    },
    {
      "epoch": 0.5439005439005439,
      "grad_norm": 9.544312477111816,
      "learning_rate": 4.456876456876457e-05,
      "loss": 0.4505,
      "step": 700
    },
    {
      "epoch": 0.6216006216006216,
      "grad_norm": 3.0595414638519287,
      "learning_rate": 4.37917637917638e-05,
      "loss": 0.4254,
      "step": 800
    },
    {
      "epoch": 0.6993006993006993,
      "grad_norm": 2.9674601554870605,
      "learning_rate": 4.301476301476302e-05,
      "loss": 0.4224,
      "step": 900
    },
    {
      "epoch": 0.777000777000777,
      "grad_norm": 1.6583951711654663,
      "learning_rate": 4.223776223776224e-05,
      "loss": 0.4152,
      "step": 1000
    },
    {
      "epoch": 0.8547008547008547,
      "grad_norm": 15.492809295654297,
      "learning_rate": 4.146076146076146e-05,
      "loss": 0.4048,
      "step": 1100
    },
    {
      "epoch": 0.9324009324009324,
      "grad_norm": 21.760177612304688,
      "learning_rate": 4.068376068376069e-05,
      "loss": 0.4388,
      "step": 1200
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 9.457627296447754,
      "learning_rate": 3.9906759906759914e-05,
      "loss": 0.4313,
      "step": 1300
    },
    {
      "epoch": 1.0878010878010878,
      "grad_norm": 6.115718364715576,
      "learning_rate": 3.912975912975913e-05,
      "loss": 0.4119,
      "step": 1400
    },
    {
      "epoch": 1.1655011655011656,
      "grad_norm": 10.92234992980957,
      "learning_rate": 3.835275835275836e-05,
      "loss": 0.42,
      "step": 1500
    },
    {
      "epoch": 1.2432012432012431,
      "grad_norm": 6.056063175201416,
      "learning_rate": 3.757575757575758e-05,
      "loss": 0.4165,
      "step": 1600
    },
    {
      "epoch": 1.320901320901321,
      "grad_norm": 3.722445249557495,
      "learning_rate": 3.67987567987568e-05,
      "loss": 0.4291,
      "step": 1700
    },
    {
      "epoch": 1.3986013986013985,
      "grad_norm": 2.9593639373779297,
      "learning_rate": 3.602175602175603e-05,
      "loss": 0.4042,
      "step": 1800
    },
    {
      "epoch": 1.4763014763014763,
      "grad_norm": 7.838626861572266,
      "learning_rate": 3.524475524475525e-05,
      "loss": 0.4122,
      "step": 1900
    },
    {
      "epoch": 1.554001554001554,
      "grad_norm": 3.286968946456909,
      "learning_rate": 3.446775446775447e-05,
      "loss": 0.4076,
      "step": 2000
    },
    {
      "epoch": 1.6317016317016317,
      "grad_norm": 9.73688793182373,
      "learning_rate": 3.369075369075369e-05,
      "loss": 0.4113,
      "step": 2100
    },
    {
      "epoch": 1.7094017094017095,
      "grad_norm": 21.8026123046875,
      "learning_rate": 3.291375291375292e-05,
      "loss": 0.4057,
      "step": 2200
    },
    {
      "epoch": 1.7871017871017871,
      "grad_norm": 24.272628784179688,
      "learning_rate": 3.2136752136752144e-05,
      "loss": 0.406,
      "step": 2300
    },
    {
      "epoch": 1.8648018648018647,
      "grad_norm": 33.69810104370117,
      "learning_rate": 3.135975135975136e-05,
      "loss": 0.3983,
      "step": 2400
    },
    {
      "epoch": 1.9425019425019425,
      "grad_norm": 6.948183536529541,
      "learning_rate": 3.058275058275059e-05,
      "loss": 0.4053,
      "step": 2500
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 11.846490859985352,
      "learning_rate": 2.9805749805749804e-05,
      "loss": 0.4041,
      "step": 2600
    },
    {
      "epoch": 2.097902097902098,
      "grad_norm": 17.608440399169922,
      "learning_rate": 2.902874902874903e-05,
      "loss": 0.4219,
      "step": 2700
    },
    {
      "epoch": 2.1756021756021755,
      "grad_norm": 12.939903259277344,
      "learning_rate": 2.8251748251748255e-05,
      "loss": 0.3871,
      "step": 2800
    },
    {
      "epoch": 2.2533022533022535,
      "grad_norm": 6.599870204925537,
      "learning_rate": 2.7474747474747474e-05,
      "loss": 0.4065,
      "step": 2900
    },
    {
      "epoch": 2.331002331002331,
      "grad_norm": 20.87743377685547,
      "learning_rate": 2.66977466977467e-05,
      "loss": 0.4079,
      "step": 3000
    },
    {
      "epoch": 2.4087024087024087,
      "grad_norm": 2.8692238330841064,
      "learning_rate": 2.592074592074592e-05,
      "loss": 0.3676,
      "step": 3100
    },
    {
      "epoch": 2.4864024864024863,
      "grad_norm": 1.6279698610305786,
      "learning_rate": 2.5143745143745145e-05,
      "loss": 0.403,
      "step": 3200
    },
    {
      "epoch": 2.564102564102564,
      "grad_norm": 17.723468780517578,
      "learning_rate": 2.4366744366744367e-05,
      "loss": 0.4143,
      "step": 3300
    },
    {
      "epoch": 2.641802641802642,
      "grad_norm": 24.460712432861328,
      "learning_rate": 2.358974358974359e-05,
      "loss": 0.3949,
      "step": 3400
    },
    {
      "epoch": 2.7195027195027195,
      "grad_norm": 17.664066314697266,
      "learning_rate": 2.281274281274281e-05,
      "loss": 0.3979,
      "step": 3500
    },
    {
      "epoch": 2.797202797202797,
      "grad_norm": 2.9844613075256348,
      "learning_rate": 2.2035742035742037e-05,
      "loss": 0.3922,
      "step": 3600
    },
    {
      "epoch": 2.874902874902875,
      "grad_norm": 7.595237731933594,
      "learning_rate": 2.125874125874126e-05,
      "loss": 0.3981,
      "step": 3700
    },
    {
      "epoch": 2.9526029526029527,
      "grad_norm": 3.5864663124084473,
      "learning_rate": 2.0481740481740482e-05,
      "loss": 0.4094,
      "step": 3800
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 15.08734130859375,
      "learning_rate": 1.9704739704739704e-05,
      "loss": 0.3742,
      "step": 3900
    },
    {
      "epoch": 3.108003108003108,
      "grad_norm": 13.524043083190918,
      "learning_rate": 1.8927738927738926e-05,
      "loss": 0.4082,
      "step": 4000
    },
    {
      "epoch": 3.185703185703186,
      "grad_norm": 7.798683166503906,
      "learning_rate": 1.8150738150738152e-05,
      "loss": 0.3648,
      "step": 4100
    },
    {
      "epoch": 3.2634032634032635,
      "grad_norm": 12.005020141601562,
      "learning_rate": 1.7373737373737375e-05,
      "loss": 0.3859,
      "step": 4200
    },
    {
      "epoch": 3.341103341103341,
      "grad_norm": 5.304004192352295,
      "learning_rate": 1.6596736596736597e-05,
      "loss": 0.4002,
      "step": 4300
    },
    {
      "epoch": 3.4188034188034186,
      "grad_norm": 4.240583419799805,
      "learning_rate": 1.581973581973582e-05,
      "loss": 0.4013,
      "step": 4400
    },
    {
      "epoch": 3.4965034965034967,
      "grad_norm": 15.891953468322754,
      "learning_rate": 1.5042735042735043e-05,
      "loss": 0.3767,
      "step": 4500
    },
    {
      "epoch": 3.5742035742035743,
      "grad_norm": 5.527275562286377,
      "learning_rate": 1.4265734265734265e-05,
      "loss": 0.4108,
      "step": 4600
    },
    {
      "epoch": 3.651903651903652,
      "grad_norm": 21.596588134765625,
      "learning_rate": 1.3488733488733491e-05,
      "loss": 0.3635,
      "step": 4700
    },
    {
      "epoch": 3.7296037296037294,
      "grad_norm": 4.571187973022461,
      "learning_rate": 1.2711732711732714e-05,
      "loss": 0.3932,
      "step": 4800
    },
    {
      "epoch": 3.8073038073038075,
      "grad_norm": 8.596707344055176,
      "learning_rate": 1.1934731934731936e-05,
      "loss": 0.3828,
      "step": 4900
    },
    {
      "epoch": 3.885003885003885,
      "grad_norm": 3.013645648956299,
      "learning_rate": 1.1157731157731158e-05,
      "loss": 0.4043,
      "step": 5000
    },
    {
      "epoch": 3.9627039627039626,
      "grad_norm": 10.341839790344238,
      "learning_rate": 1.0380730380730382e-05,
      "loss": 0.4113,
      "step": 5100
    },
    {
      "epoch": 4.040404040404041,
      "grad_norm": 6.298069000244141,
      "learning_rate": 9.603729603729605e-06,
      "loss": 0.3871,
      "step": 5200
    },
    {
      "epoch": 4.118104118104118,
      "grad_norm": 3.6584420204162598,
      "learning_rate": 8.826728826728829e-06,
      "loss": 0.3748,
      "step": 5300
    },
    {
      "epoch": 4.195804195804196,
      "grad_norm": 14.74254035949707,
      "learning_rate": 8.04972804972805e-06,
      "loss": 0.3773,
      "step": 5400
    },
    {
      "epoch": 4.273504273504273,
      "grad_norm": 4.68160343170166,
      "learning_rate": 7.272727272727272e-06,
      "loss": 0.377,
      "step": 5500
    },
    {
      "epoch": 4.351204351204351,
      "grad_norm": 13.33260440826416,
      "learning_rate": 6.495726495726496e-06,
      "loss": 0.3886,
      "step": 5600
    },
    {
      "epoch": 4.428904428904429,
      "grad_norm": 25.142353057861328,
      "learning_rate": 5.718725718725719e-06,
      "loss": 0.4234,
      "step": 5700
    },
    {
      "epoch": 4.506604506604507,
      "grad_norm": 12.797850608825684,
      "learning_rate": 4.941724941724942e-06,
      "loss": 0.3575,
      "step": 5800
    },
    {
      "epoch": 4.584304584304585,
      "grad_norm": 10.379070281982422,
      "learning_rate": 4.164724164724165e-06,
      "loss": 0.3836,
      "step": 5900
    },
    {
      "epoch": 4.662004662004662,
      "grad_norm": 6.395726680755615,
      "learning_rate": 3.3877233877233877e-06,
      "loss": 0.4044,
      "step": 6000
    },
    {
      "epoch": 4.73970473970474,
      "grad_norm": 8.953837394714355,
      "learning_rate": 2.610722610722611e-06,
      "loss": 0.3732,
      "step": 6100
    },
    {
      "epoch": 4.817404817404817,
      "grad_norm": 6.950634956359863,
      "learning_rate": 1.833721833721834e-06,
      "loss": 0.4048,
      "step": 6200
    },
    {
      "epoch": 4.895104895104895,
      "grad_norm": 11.020462036132812,
      "learning_rate": 1.0567210567210568e-06,
      "loss": 0.3907,
      "step": 6300
    },
    {
      "epoch": 4.972804972804973,
      "grad_norm": 3.348013401031494,
      "learning_rate": 2.7972027972027973e-07,
      "loss": 0.4019,
      "step": 6400
    }
  ],
  "logging_steps": 100,
  "max_steps": 6435,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 172894443724800.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
