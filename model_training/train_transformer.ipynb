{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26ecd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb24d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalyani\\Music\\MSIAI5001\\Project\\MSI5001 Group Project\\MSI5001-Group-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import string\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d4849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parse FASTA\n",
    "# Function to load sequences from a FASTA file\n",
    "def load_fasta(path):\n",
    "    seqs = {}                      # Dictionary to store ID: sequence pairs\n",
    "    with open(path) as f:\n",
    "        seq_id = None              # Variable to hold current sequence ID\n",
    "        seq = ''                   # Variable to build sequence string\n",
    "        for line in f:\n",
    "            if line.startswith('>'):       # Header line â†’ new sequence starts\n",
    "                if seq_id:                 # If previous sequence exists, save it\n",
    "                    seqs[seq_id] = seq\n",
    "                seq_id = line[1:].strip()  # Extract ID (remove '>')\n",
    "                seq = ''                   # Reset sequence for next entry\n",
    "            else:\n",
    "                seq += line.strip().upper() # Add sequence line (uppercase)\n",
    "        if seq_id:                         # Save last sequence after loop ends\n",
    "            seqs[seq_id] = seq\n",
    "    return seqs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c801441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22867 sequences.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load data\n",
    "# ------------------------------\n",
    "fasta_path = r'C:\\Users\\Kalyani\\Music\\MSIAI5001\\Project\\MSI5001 Group Project\\MSI5001-Group-Project\\dataset\\training.fa'\n",
    "labels_path = r'C:\\Users\\Kalyani\\Music\\MSIAI5001\\Project\\MSI5001 Group Project\\MSI5001-Group-Project\\dataset\\training_class.csv'\n",
    "\n",
    "seq_dict = load_fasta(fasta_path)\n",
    "df = pd.read_csv(labels_path, names=['id', 'label'])  # assuming no header in CSV\n",
    "df['sequence'] = df['id'].map(seq_dict)\n",
    "df = df.dropna()\n",
    "\n",
    "# Ensure labels are numeric\n",
    "df['label'] = pd.Categorical(df['label']).codes\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(df)} sequences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1ae243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train/val split\n",
    "# ------------------------------\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, stratify=df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330cac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenizer (character-level)\n",
    "# ------------------------------\n",
    "all_chars = set(''.join(df['sequence'].values))\n",
    "char_vocab = sorted(list(all_chars))\n",
    "vocab_dict = {ch: idx + 2 for idx, ch in enumerate(char_vocab)}  # +2 to reserve 0 (pad), 1 (unk)\n",
    "vocab_dict['[PAD]'] = 0\n",
    "vocab_dict['[UNK]'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a191f",
   "metadata": {},
   "source": [
    "# This class implements a simple character-level tokenizer:\n",
    "# - Converts RNA sequence characters to numeric IDs.\n",
    "# - Supports padding and truncation for uniform input size.\n",
    "# - Can decode token IDs back to sequences.\n",
    "# - Provides save/load functionality (like Hugging Face tokenizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCharTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab ## Character-to-ID mapping\n",
    "        self.ids_to_tokens = {i: t for t, i in vocab.items()} \n",
    "        self.pad_token = '[PAD]'\n",
    "        self.unk_token = '[UNK]'\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "\n",
    "    def encode(self, text, max_length=512, padding='max_length', truncation=True):\n",
    "        tokens = [self.vocab.get(ch, self.unk_token_id) for ch in text]\n",
    "        if truncation:\n",
    "            tokens = tokens[:max_length]\n",
    "        if padding == 'max_length':\n",
    "            tokens = tokens + [self.pad_token_id] * max(0, max_length - len(tokens))\n",
    "        return tokens\n",
    "# - Can decode token IDs back to sequences.\n",
    "    def decode(self, token_ids):\n",
    "        return ''.join([self.ids_to_tokens.get(i, self.unk_token) for i in token_ids])\n",
    "# - Provides save/load functionality (like Hugging Face tokenizers).\n",
    "    \n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"Save vocab to a directory (so Trainer can checkpoint).\"\"\"\n",
    "        \n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        vocab_path = os.path.join(save_directory, \"vocab.json\")\n",
    "        with open(vocab_path, \"w\") as f:\n",
    "            json.dump(self.vocab, f)\n",
    "        print(f\"Tokenizer saved to {vocab_path}\")\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_directory):\n",
    "        \"\"\"Load vocab from a saved directory.\"\"\"\n",
    "        import json, os\n",
    "        vocab_path = os.path.join(load_directory, \"vocab.json\")\n",
    "        with open(vocab_path, \"r\") as f:\n",
    "            vocab = json.load(f)\n",
    "        print(f\" Tokenizer loaded from {vocab_path}\")\n",
    "        return cls(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59d2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this tokenizer\n",
    "tokenizer = SimpleCharTokenizer(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c9c6d",
   "metadata": {},
   "source": [
    "# The RNASequenceDataset class:\n",
    "# - Converts RNA sequences and labels from a dataframe into model-ready tensors.\n",
    "# - Uses a tokenizer to encode sequences into token IDs with padding/truncation.\n",
    "# - Creates attention masks (1 = token, 0 = padding).\n",
    "# - Returns a dictionary compatible with PyTorch DataLoader for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19debab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Dataset Classes\n",
    "# ------------------------------\n",
    "\n",
    "class RNASequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=512):\n",
    "        self.data = dataframe.reset_index(drop=True) \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data.loc[idx, 'sequence']\n",
    "        label = self.data.loc[idx, 'label']\n",
    "        tokens = self.tokenizer.encode(seq, max_length=self.max_len, padding='max_length', truncation=True)\n",
    "        attn_mask = [1 if t != self.tokenizer.pad_token_id else 0 for t in tokens]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf57d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNATestDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=512):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data.loc[idx, 'sequence']\n",
    "        tokens = [self.tokenizer.vocab.get(ch, 1) for ch in seq[:self.max_len]]\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens = tokens + [0] * pad_len\n",
    "        attn_mask = [1 if t != 0 else 0 for t in tokens]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attn_mask, dtype=torch.long)\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a895bf8",
   "metadata": {},
   "source": [
    "    # The RNATestDataset class:\n",
    "# - Prepares RNA sequences (without labels) for model inference.\n",
    "# - Converts each character to its corresponding token ID using the tokenizer.\n",
    "# - Pads or truncates sequences to a fixed length.\n",
    "# - Creates an attention mask (1 for tokens, 0 for padding).\n",
    "# - Returns tensors ready for PyTorch DataLoader during testing or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Config\n",
    "# ------------------------------\n",
    "vocab_size = len(vocab_dict)\n",
    "config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=256,\n",
    "    max_position_embeddings=512,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86805e",
   "metadata": {},
   "source": [
    "# - Defines a BERT configuration for RNA sequence classification.\n",
    "# - vocab_size: number of unique characters (tokens) in the vocabulary.\n",
    "# - hidden_size: dimension of hidden embeddings.\n",
    "# - num_hidden_layers: number of Transformer encoder layers.\n",
    "# - num_attention_heads: number of self-attention heads per layer.\n",
    "# - intermediate_size: size of the feed-forward layer inside each block.\n",
    "# - max_position_embeddings: maximum sequence length (512 tokens).\n",
    "# - num_labels: number of output classes for classification.\n",
    "# - Initializes a BERT model (BertForSequenceClassification) with this config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ff2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prepare datasets\n",
    "# ------------------------------\n",
    "train_dataset = RNASequenceDataset(train_df, tokenizer)\n",
    "val_dataset = RNASequenceDataset(val_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training arguments\n",
    "# ------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./rna-transformer',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,      # log every 100 steps\n",
    "    save_total_limit=2,\n",
    "    do_eval=True,           # enable evaluation\n",
    "    do_train=True,          # enable training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Compute metrics\n",
    "# ------------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn + 1e-8)\n",
    "    specificity = tn / (tn + fp + 1e-8)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'mcc': mcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598207d",
   "metadata": {},
   "source": [
    "# The compute_metrics function calculates evaluation metrics for classification:\n",
    "# - accuracy: overall correct predictions\n",
    "# - precision: fraction of predicted positives that are correct\n",
    "# - recall (sensitivity): fraction of actual positives correctly identified\n",
    "# - f1: harmonic mean of precision and recall\n",
    "# - specificity: fraction of actual negatives correctly identified\n",
    "# - mcc: Matthews correlation coefficient (balanced measure for binary classes)\n",
    "# - Uses confusion matrix to compute sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54222d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Trainer setup\n",
    "# ------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# 11. Train\n",
    "# ------------------------------\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a072680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Evaluate\n",
    "# ------------------------------\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\n Evaluation Results:\")\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Test predictions\n",
    "# ------------------------------\n",
    "test_df = pd.read_csv(r'dataset\\test.csv')\n",
    "\n",
    "# If test.csv has only sequences\n",
    "if 'sequence' not in test_df.columns:\n",
    "    test_df.columns = ['sequence']\n",
    "\n",
    "test_dataset = RNATestDataset(test_df, tokenizer)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "test_df['predicted_label'] = predicted_labels\n",
    "test_df.to_csv('result\\test_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'test_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20403d6",
   "metadata": {},
   "source": [
    "# - Loads test sequences from a CSV file.\n",
    "# - Ensures the dataframe has a 'sequence' column.\n",
    "# - Creates an RNATestDataset for model inference.\n",
    "# - Uses the trainer to predict sequence classes.\n",
    "# - Converts model outputs to predicted class labels.\n",
    "# - Appends predictions to the dataframe and saves to CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880563c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
